from xmlrpc.client import Boolean

from geh_common.pyspark.data_frame_wrapper import DataFrameWrapper
from pyspark.sql import DataFrame
from pyspark.sql import types as T

from geh_calculated_measurements.common.domain.column_names import ContractColumnNames


class CalculatedMeasurementsDaily(DataFrameWrapper):
    """Data type for daily calculated measurements generated by calculations."""

    # Define schema as a class attribute so it's accessible without instantiation
    schema = T.StructType(
        [
            T.StructField(ContractColumnNames.metering_point_id, T.StringType(), False),
            T.StructField(ContractColumnNames.date, T.TimestampType(), False),
            T.StructField(ContractColumnNames.quantity, T.DecimalType(18, 3), False),
            T.StructField(ContractColumnNames.settlement_type, T.StringType(), True),  # Default nullable
        ]
    )

    def __init__(self, df: DataFrame, settlement_type_nullable: Boolean = True) -> None:
        # If non-default nullability is requested, create a modified schema for this instance
        if not settlement_type_nullable:
            instance_schema = T.StructType(
                [
                    T.StructField(ContractColumnNames.metering_point_id, T.StringType(), False),
                    T.StructField(ContractColumnNames.date, T.TimestampType(), False),
                    T.StructField(ContractColumnNames.quantity, T.DecimalType(18, 3), False),
                    T.StructField(ContractColumnNames.settlement_type, T.StringType(), False),
                ]
            )
        else:
            instance_schema = self.schema

        super().__init__(
            df=df,
            schema=instance_schema,
            ignore_nullability=True,
        )
